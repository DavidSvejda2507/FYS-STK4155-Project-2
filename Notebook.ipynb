{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYS-STK project 2\\\n",
    "By David Svejda, Gianmarco Puleo and Henrik Breitenstein\n",
    "\n",
    "All of the results are generated from scripts in this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding suitable parameter values\n",
    "\n",
    "We start by optimising the learningrate, the number of epochs and the regularisation parameter lambda for stochastic gradient descent. First we optimise the learning rate and number of epoch with a small regularisation parameter by looking at how the accuracy and cross-entropy changes based on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import NeuralNet as NN\n",
    "import optimisers as op\n",
    "import ActivationFunctions as AF\n",
    "import pandas as pd\n",
    "import Data\n",
    "\n",
    "\n",
    "ln10 = np.log(10)\n",
    "\n",
    "def Accuracy(predictions, targets):\n",
    "    numbers = np.arange(0, 10, 1)\n",
    "    predicted_values = numbers[np.argmax(predictions, axis=1)]\n",
    "    diff = np.equal(predicted_values, targets)\n",
    "    return diff**2, diff*2\n",
    "\n",
    "def Cross_Entropy(predictions, targets):\n",
    "    predictions = predictions.T\n",
    "    expanded = np.expand_dims(targets, axis=1)\n",
    "    predicted_values = np.reshape(np.take_along_axis(predictions, expanded, axis=1), (len(predictions), ))\n",
    "    error = -np.log10(abs(predicted_values)+1e-8)\n",
    "    derror = -1/(abs(predicted_values*ln10)+1e-8)\n",
    "    zeros = np.zeros((10, len(predicted_values)))\n",
    "    i = 0\n",
    "    for k, j in zip(derror, targets):\n",
    "        zeros[j, i] = k\n",
    "        i += 1\n",
    "    return error, zeros\n",
    "\n",
    "def lr_ep_error(n_epochs, nr_batches, inputs, targets, test_data, test_targets, costFunc, model):\n",
    "    epochs = 0\n",
    "    while epochs <= n_epochs:\n",
    "        batches = np.array_split(inputs, nr_batches, axis=1)\n",
    "        batches_targets = np.array_split(targets, nr_batches, axis=0)\n",
    "        for batch_nr in range(nr_batches):\n",
    "            rand_n = np.random.choice(range(nr_batches))\n",
    "            model.back_propagate(batches[rand_n], batches_targets[rand_n], costFunc)\n",
    "        epochs += 1\n",
    "    predictions = model.feed_forward(test_data)\n",
    "    Acc = Accuracy(predictions.T, test_targets)[0].mean()\n",
    "    Cross_Ent = Cross_Entropy(predictions, test_targets)[0].mean()\n",
    "    return Acc, Cross_Ent\n",
    "\n",
    "def FixedLambda(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, Lmd):\n",
    "    min = 1e8\n",
    "    max = 0\n",
    "\n",
    "    lr_ep_Cross_ent = np.zeros((len(lr_range)+1, len(ep_range)+1))\n",
    "    lr_ep_Acc = np.zeros((len(lr_range)+1, len(ep_range)+1))\n",
    "    lr_ep_Cross_ent[1:, 0] = lr_range\n",
    "    lr_ep_Cross_ent[0, 1:] = ep_range\n",
    "    lr_ep_Acc[1:, 0] = lr_range\n",
    "    lr_ep_Acc[0, 1:] = ep_range\n",
    "\n",
    "    for k, lr in enumerate(lr_range):\n",
    "        print('.', end='', flush=True)\n",
    "        for j, n_epoch in enumerate(ep_range):\n",
    "            print(',', end='', flush=True)\n",
    "            model = NN.Model(shapes, [af]*(len(shapes)-1), opt(lr, 1e-8, lamda=Lmd))\n",
    "\n",
    "            inputs = data\n",
    "\n",
    "            Acc, Cross_Ent = lr_ep_error(n_epoch, nr_batches, inputs, targets, test_data, test_targets, costFunc, model)\n",
    "\n",
    "            lr_ep_Cross_ent[k+1, j+1] = Cross_Ent\n",
    "            lr_ep_Acc[k+1, j+1] = Acc\n",
    "\n",
    "            if Acc>max:\n",
    "                max = Acc\n",
    "                acc_entropy = Cross_Ent\n",
    "                k_acc_min = k\n",
    "                j_acc_min = j\n",
    "\n",
    "            if Cross_Ent<min:\n",
    "                min = Cross_Ent\n",
    "                Cross_acc = Acc\n",
    "                k_min = k\n",
    "                j_min = j\n",
    "\n",
    "    print('\\n')\n",
    "    #k = lr, j = epochs\n",
    "    return [k_min, j_min, min, Cross_acc, k_acc_min, j_acc_min, max, acc_entropy, lr_ep_Cross_ent, lr_ep_Cross_ent]\n",
    "\n",
    "\n",
    "def Run(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, name, Lmd):\n",
    "    RL = FixedLambda(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, Lmd)\n",
    "    print(f'Momentum: {L}, Lr: {lr_range[RL[0]]} , Epochs: {ep_range[RL[1]]}')\n",
    "    Acc_Image = RL[-2]\n",
    "    Ent_Image = RL[-1]\n",
    "    np.save(f'./Data/NrHidden{len(shapes)-2}/{opt.__name__}/LrEpoch/Acc_{name}', Acc_Image)\n",
    "    np.save(f'./Data/NrHidden{len(shapes)-2}/{opt.__name__}/LrEpoch/Ent_{name}', Ent_Image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum\n",
    "\n",
    "Before optimizing the learning rate and number of epochs we need a suitable momentum. To find this we do a small test of different values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixedLambda(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, Lmd):\n",
    "    min = 1e8\n",
    "    max = 0\n",
    "\n",
    "    lr_ep_Cross_ent = np.zeros((len(lr_range)+1, len(ep_range)+1))\n",
    "    lr_ep_Acc = np.zeros((len(lr_range)+1, len(ep_range)+1))\n",
    "    lr_ep_Cross_ent[1:, 0] = lr_range\n",
    "    lr_ep_Cross_ent[0, 1:] = ep_range\n",
    "    lr_ep_Acc[1:, 0] = lr_range\n",
    "    lr_ep_Acc[0, 1:] = ep_range\n",
    "\n",
    "    for k, lr in enumerate(lr_range):\n",
    "        print('.', end='', flush=True)\n",
    "        for j, n_epoch in enumerate(ep_range):\n",
    "            print(',', end='', flush=True)\n",
    "            optimiser = opt(lr, momentum=L,lamda=Lmd)\n",
    "            #model = NN.Model(shapes, [af]*(len(shapes)-1), op.LrScheduleOptimiser(schedule(lr, t), optimiser))\n",
    "            model = NN.Model(shapes, [af]*(len(shapes)-1), optimiser)\n",
    "            inputs = data\n",
    "\n",
    "            Acc, Cross_Ent = lr_ep_error(n_epoch, nr_batches, inputs, targets, test_data, test_targets, costFunc, model)\n",
    "\n",
    "            lr_ep_Cross_ent[k+1, j+1] = Cross_Ent\n",
    "            lr_ep_Acc[k+1, j+1] = Acc\n",
    "\n",
    "            if Acc>max:\n",
    "                max = Acc\n",
    "                acc_entropy = Cross_Ent\n",
    "                k_acc_min = k\n",
    "                j_acc_min = j\n",
    "\n",
    "            if Cross_Ent<min:\n",
    "                min = Cross_Ent\n",
    "                Cross_acc = Acc\n",
    "                k_min = k\n",
    "                j_min = j\n",
    "\n",
    "    print('\\n')\n",
    "    #k = lr, j = epochs\n",
    "    return [k_min, j_min, min, Cross_acc, k_acc_min, j_acc_min, max, acc_entropy, lr_ep_Cross_ent, lr_ep_Cross_ent]\n",
    "\n",
    "\n",
    "def Run(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, name, Lmd):\n",
    "    RL = FixedLambda(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, Lmd)\n",
    "    Acc_Image = RL[-2]\n",
    "    Ent_Image = RL[-1]\n",
    "    min = RL[2]\n",
    "    max = RL[6]\n",
    "    print(f'Momentum: {L}, Lr: {lr_range[RL[0]]} , Epochs: {ep_range[RL[1]]}, Best Acc: {max}, Best CE: {min}')\n",
    "    # np.save(f'./Data/NrHidden{len(shapes)-2}/{opt.__name__}/LrEpoch/Acc_{name}', Acc_Image)\n",
    "    # np.save(f'./Data/NrHidden{len(shapes)-2}/{opt.__name__}/LrEpoch/Ent_{name}', Ent_Image)\n",
    "\n",
    "\n",
    "\n",
    "Llist = np.arange(1, 4, 0.5)\n",
    "Lmd = 1e-4\n",
    "[lr_range, ep_range] = [np.logspace(-4, 0, 5), np.logspace(1, 2, 5)]\n",
    "shapes = (64, 10)\n",
    "train, test, val, train_tar, test_tar, val_tar = Data.load_data()\n",
    "for L in Llist:\n",
    "    name = ''\n",
    "    Run(L, lr_range, ep_range, 22, train, train_tar, test, test_tar, Cross_Entropy, shapes, AF.SoftMax(), op.MomentumOptimiser, name, Lmd)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "Momentum: 1.0, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9192200557103064, Best CE: 0.19783340865997112\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "Momentum: 1.5, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.9331476323119777, Best CE: 0.15110786614432806\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "Momentum: 2.0, Lr: 0.01 , Epochs: 100.0, Best Acc: 0.9331476323119777, Best CE: 0.1501682733652741\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "Momentum: 2.5, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.9303621169916435, Best CE: 0.15304690593490117\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "Momentum: 3.0, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9331476323119777, Best CE: 0.1747076300443229\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "Momentum: 3.5, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9387186629526463, Best CE: 0.17860384097335127"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where we choose $\\text{Momentum} = 1.5$ since this had one of the best accuracies whith a low number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Schedule\n",
    "\n",
    "For optimizing parameters together with a learning rate schedule we first do a test on a smaller range of parameters to find a suitable value for the number of steps before the learning rate is halved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixedLambda(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, Lmd, schedule, t):\n",
    "    min = 1e8\n",
    "    max = 0\n",
    "\n",
    "    lr_ep_Cross_ent = np.zeros((len(lr_range)+1, len(ep_range)+1))\n",
    "    lr_ep_Acc = np.zeros((len(lr_range)+1, len(ep_range)+1))\n",
    "    lr_ep_Cross_ent[1:, 0] = lr_range\n",
    "    lr_ep_Cross_ent[0, 1:] = ep_range\n",
    "    lr_ep_Acc[1:, 0] = lr_range\n",
    "    lr_ep_Acc[0, 1:] = ep_range\n",
    "\n",
    "    for k, lr in enumerate(lr_range):\n",
    "        print('.', end='', flush=True)\n",
    "        for j, n_epoch in enumerate(ep_range):\n",
    "            print(',', end='', flush=True)\n",
    "            optimiser = opt(lr, lamda=Lmd)\n",
    "            model = NN.Model(shapes, [af]*(len(shapes)-1), op.LrScheduleOptimiser(schedule(lr, t), optimiser))\n",
    "\n",
    "            inputs = data\n",
    "\n",
    "            Acc, Cross_Ent = lr_ep_error(n_epoch, nr_batches, inputs, targets, test_data, test_targets, costFunc, model)\n",
    "\n",
    "            lr_ep_Cross_ent[k+1, j+1] = Cross_Ent\n",
    "            lr_ep_Acc[k+1, j+1] = Acc\n",
    "\n",
    "            if Acc>max:\n",
    "                max = Acc\n",
    "                acc_entropy = Cross_Ent\n",
    "                k_acc_min = k\n",
    "                j_acc_min = j\n",
    "\n",
    "            if Cross_Ent<min:\n",
    "                min = Cross_Ent\n",
    "                Cross_acc = Acc\n",
    "                k_min = k\n",
    "                j_min = j\n",
    "\n",
    "    print('\\n')\n",
    "    #k = lr, j = epochs\n",
    "    return [k_min, j_min, min, Cross_acc, k_acc_min, j_acc_min, max, acc_entropy, lr_ep_Cross_ent, lr_ep_Cross_ent]\n",
    "\n",
    "\n",
    "\n",
    "def Run(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, name, Lmd, t, schedule):\n",
    "    RL = FixedLambda(L, lr_range, ep_range, nr_batches, data, targets, test_data, test_targets, costFunc, shapes, af, opt, Lmd, schedule, t)\n",
    "    print(f't1: {t}, Lr: {lr_range[RL[0]]} , Epochs: {ep_range[RL[1]]}')\n",
    "    Acc_Image = RL[-2]\n",
    "    Ent_Image = RL[-1]\n",
    "    # np.save(f'./Data/NrHidden{len(shapes)-2}/{opt.__name__}/LrEpoch/Acc_{name}', Acc_Image)\n",
    "    # np.save(f'./Data/NrHidden{len(shapes)-2}/{opt.__name__}/LrEpoch/Ent_{name}', Ent_Image)\n",
    "\n",
    "\n",
    "\n",
    "L = 3\n",
    "Lmd = 1e-4\n",
    "[lr_range, ep_range, t1] = [np.logspace(-4, 0, 3), np.logspace(1, 2, 3), np.logspace(2, 0, 10)]\n",
    "shapes = (64, 10)\n",
    "train, test, val, train_tar, test_tar, val_tar = Data.load_data()\n",
    "schedules = [lrs.hyperbolic_lr, lrs.linear_lr, lrs.exponential_lr]\n",
    "opts = [op.Optimiser, op.MomentumOptimiser]\n",
    "for opt in opts:\n",
    "    for schedule in schedules:\n",
    "        print(f'{schedule.__name__:-^20}')\n",
    "        for t in t1:\n",
    "            name = f'L_{L}lamda{Lmd}Sch{schedule}'\n",
    "            Run(L, lr_range, ep_range, 22, train, train_tar, test, test_tar, Cross_Entropy, shapes, AF.SoftMax(), opt, name, Lmd, t, schedule)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Optimiser:\n",
    "---hyperbolic_lr----\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 1000.0, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9331476323119777, Best CE: 0.13847945173053677\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 774.263682681127, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9108635097493036, Best CE: 0.22386061404137386\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 599.4842503189409, Lr: 0.01 , Epochs: 100.0, Best Acc: 0.8467966573816156, Best CE: 0.39681569367659225\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 464.15888336127773, Lr: 0.01 , Epochs: 10.0, Best Acc: 0.8440111420612814, Best CE: 0.6389040936446235\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 359.38136638046257, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9108635097493036, Best CE: 0.1781169927005575\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 278.2559402207126, Lr: 0.01 , Epochs: 100.0, Best Acc: 0.8161559888579387, Best CE: 0.4718195353640893\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 215.44346900318845, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9387186629526463, Best CE: 0.16414673659135062\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 166.81005372000593, Lr: 0.01 , Epochs: 100.0, Best Acc: 0.8245125348189415, Best CE: 0.3590460035299426\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 129.1549665014884, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.9192200557103064, Best CE: 0.18713587483346342\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 100.0, Lr: 0.1 , Epochs: 31.622776601683793, Best Acc: 0.9164345403899722, Best CE: 0.1665030972216602\n",
    "-----linear_lr------\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 1000.0, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.9052924791086351, Best CE: 0.21896673608313563\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 774.263682681127, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.9220055710306406, Best CE: 0.16642322956184527\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 599.4842503189409, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.924791086350975, Best CE: 0.189901784918876\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 464.15888336127773, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.9220055710306406, Best CE: 0.2112225347369133\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 359.38136638046257, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.9136490250696379, Best CE: 0.17954519205097938\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 278.2559402207126, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.9164345403899722, Best CE: 0.22202147096811595\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 215.44346900318845, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.596100278551532, Best CE: 1.9476959754365402\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 166.81005372000593, Lr: 0.001 , Epochs: 10.0, Best Acc: 0.181058495821727, Best CE: 5.201197849403617\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 129.1549665014884, Lr: 1.0 , Epochs: 31.622776601683793, Best Acc: 0.28969359331476324, Best CE: 5.650675378141172\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 100.0, Lr: 0.0001 , Epochs: 10.0, Best Acc: 0.12256267409470752, Best CE: 6.406864327498341\n",
    "---exponential_lr---\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 1000.0, Lr: 0.01 , Epochs: 56.23413251903491, Best Acc: 0.883008356545961, Best CE: 0.21880095412434916\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 774.263682681127, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9526462395543176, Best CE: 0.12000873099560569\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 599.4842503189409, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.924791086350975, Best CE: 0.23190578413076926\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 464.15888336127773, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9220055710306406, Best CE: 0.19060212195067086\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 359.38136638046257, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9220055710306406, Best CE: 0.1849189386362163\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 278.2559402207126, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9192200557103064, Best CE: 0.16557974341711865\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 215.44346900318845, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9303621169916435, Best CE: 0.1992486783293667\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 166.81005372000593, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.8997214484679665, Best CE: 0.2757350257219133\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 129.1549665014884, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.9108635097493036, Best CE: 0.22207014519756824\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 100.0, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.8913649025069638, Best CE: 0.24984680105662377\n",
    "MomentumOptimiser:\n",
    "---hyperbolic_lr----\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 1000.0, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9415041782729805, Best CE: 0.14936071018202396\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 774.263682681127, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9164345403899722, Best CE: 0.23950351531518005\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 599.4842503189409, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.924791086350975, Best CE: 0.17989266222746586\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 464.15888336127773, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9442896935933147, Best CE: 0.15806369441633303\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 359.38136638046257, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9303621169916435, Best CE: 0.1572985889188011\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 278.2559402207126, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9303621169916435, Best CE: 0.1393511103848668\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 215.44346900318845, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9303621169916435, Best CE: 0.15724989163572228\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 166.81005372000593, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.9108635097493036, Best CE: 0.2182406759533315\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 129.1549665014884, Lr: 0.1 , Epochs: 31.622776601683793, Best Acc: 0.9275766016713092, Best CE: 0.17727785718367078\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 100.0, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9303621169916435, Best CE: 0.1462219781257215\n",
    "-----linear_lr------\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 1000.0, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.9108635097493036, Best CE: 0.2081605478486477\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 774.263682681127, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.9331476323119777, Best CE: 0.1645209357653001\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 599.4842503189409, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.9164345403899722, Best CE: 0.2574818842496322\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 464.15888336127773, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.807799442896936, Best CE: 1.0613927325093335\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 359.38136638046257, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.8941504178272981, Best CE: 0.286979798915545\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 278.2559402207126, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.7883008356545961, Best CE: 1.0187013597977406\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 215.44346900318845, Lr: 0.1 , Epochs: 10.0, Best Acc: 0.7103064066852368, Best CE: 1.8425594683669209\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 166.81005372000593, Lr: 0.01 , Epochs: 10.0, Best Acc: 0.3565459610027855, Best CE: 4.06208153881476\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 129.1549665014884, Lr: 0.0001 , Epochs: 17.78279410038923, Best Acc: 0.20334261838440112, Best CE: 6.139182091446171\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 100.0, Lr: 0.0001 , Epochs: 10.0, Best Acc: 0.116991643454039, Best CE: 6.034164668933936\n",
    "---exponential_lr---\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 1000.0, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9220055710306406, Best CE: 0.15757000927154807\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 774.263682681127, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9220055710306406, Best CE: 0.2014589207679892\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 599.4842503189409, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9275766016713092, Best CE: 0.14740893362736127\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 464.15888336127773, Lr: 0.1 , Epochs: 31.622776601683793, Best Acc: 0.9164345403899722, Best CE: 0.24201868855804387\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 359.38136638046257, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9108635097493036, Best CE: 0.20865352811189314\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 278.2559402207126, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9275766016713092, Best CE: 0.15975076374112898\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 215.44346900318845, Lr: 0.1 , Epochs: 17.78279410038923, Best Acc: 0.9387186629526463, Best CE: 0.13905481283633994\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 166.81005372000593, Lr: 0.1 , Epochs: 31.622776601683793, Best Acc: 0.8969359331476323, Best CE: 0.2710672190401019\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 129.1549665014884, Lr: 0.1 , Epochs: 100.0, Best Acc: 0.9052924791086351, Best CE: 0.2164245904171692\n",
    ".,,,,,.,,,,,.,,,,,.,,,,,.,,,,,\n",
    "\n",
    "t1: 100.0, Lr: 0.1 , Epochs: 56.23413251903491, Best Acc: 0.9108635097493036, Best CE: 0.23775877235700274\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whith the one giving best result highlighted\n",
    "\n",
    "Normal optimiser:\n",
    "\n",
    "**hyperbolic schedule:** $\\Theta = 215$ \n",
    "\n",
    "linear schedule: $\\Theta = 774$\n",
    "\n",
    "exponential schedule: $\\Theta = 774$\n",
    "\n",
    "With momentum:\n",
    "\n",
    "**hyperbolic schedule:** $\\Theta = 464$\n",
    "\n",
    "linear schedule: $\\Theta = 774$\n",
    "\n",
    "exponential schedule: $\\Theta = 215$\n",
    "\n",
    "We are then ready to do the optimazation for five different optimisers: Momentum Optimiser, AdaGrad, Adam, RMSProp and Learning Rate Schedule optimiser. First we take a look at the results from each optimization step for the optimiser with momentum. The momentum was set to $3$ after testing a small range of momentums. With theregularisation parameter set to $\\lambda = 10^{-4}$ we get the following heat map for accuracy and cross-entropy respectivly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "53336ab67521ee4cf30c357767450a5e815ea235a46dd7a7be825542b670afa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
